\documentclass[10pt]{article}
\usepackage{soul}
\usepackage{fullpage,course}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{graphicx} % Required for inserting images
\usepackage[T1]{fontenc}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{fancyvrb}

\def\handout{Homework 1}
\parskip 6pt
\parindent 0pt

\begin{document}

\vspace*{-0.2in}

\begin{center}
\bf Student Name: Sishen Zhang \hspace{3mm} NetID: sishenz2
\end{center}

\normalsize

\section{FGSM Attack}

\subsection{}

There are three steps to implement the targeted fast gradient sign method (FGSM) attack:
\begin{enumerate}
    \item Compute perturbation: $\eta = \epsilon \cdot \text{sign}(\nabla_x \text{loss}_t(x))$ where $\text{loss}_t$ is the loss function for the target class.
    \item Generate adversarial example: $x_{adv} = x - \eta$
    \item Check if the adversarial example is classified as the target class.
\end{enumerate}

I follow the above steps to implement the targeted FGSM attack in the provided \texttt{fgsm.py} file. The critical part of the code is shown below:

\begin{Verbatim}[frame=single]
loss = L(N(x), torch.tensor([t], dtype=torch.long))

# I first calculate the gradient of the loss for the target class w.r.t. the input 
loss.backward()

# Then I compute the perturbation and generate the adversarial example
adv_x = x - eps * x.grad.sign()
\end{Verbatim}

An excerpt of the output is as follows:

\begin{Verbatim}[frame=single]
Original Class:  2
Original Input:  tensor([[0.6584, 0.8175, 0.1262, 0.0541, 0.2268, 0.8405, 0.5393, 0.9798, 
        0.8597, 0.5977]], requires_grad=True)
Adversarial Example:  tensor([[ 0.1584,  1.3175,  0.6262, -0.4459,  0.7268,  0.3405,  0.0393,
        0.4798, 1.3597,  0.0977]], grad_fn=<SubBackward0>)
New Class:  0
Norm:  tensor(0.5000)
\end{Verbatim}

The new class is exactly the target class, meaning that the attack is successful. The $L_\infty$ norm of the perturbation is 0.5, which is equal to $\epsilon$.

\newpage
\subsection{}

When I change the target class to 1, the predicted class of the adversarial example is still 2, which suggests that the attack is not successful. This does not necessarily mean that the attack becomes ineffective. It is possible that this input is far away from the decision boundary between class 2 and class 1. Therefore, the magnitude of the perturbation (i.e., $\epsilon$) may not be large enough. To verify this hypothesis, I increase $\epsilon$ from 0.5 to 1.14 and run the attack again. This time, the attack succeeds. The output is as follows:

\begin{Verbatim}[frame=single]
Original Class:  2
Gradient:  tensor([[ 0.0420, -0.0004, -0.0112,  0.0155,  0.0447, -0.0180,  0.0227,  0.0003,
         -0.0204,  0.0019]])
Original Input:  tensor([[0.6584, 0.8175, 0.1262, 0.0541, 0.2268, 0.8405, 0.5393, 0.9798, 
        0.8597, 0.5977]], requires_grad=True)
Adversarial Example:  tensor([[-0.4816,  1.9575,  1.2662, -1.0859, -0.9132,  1.9805, -0.6007,
        -0.1602, 1.9997, -0.5423]], grad_fn=<SubBackward0>)
New Class:  1
Norm:  tensor(1.1400)
\end{Verbatim}

The problem is, however, that the $L_\infty$ norm of the perturbation is now 1.1400, while the absolute value of original input is at most 1. This means that the adversarial example is probably not very close to the original input.

In view of this, I try to apply the projected gradient descent (PGD) method to generate an adversarial example that is classified as class 1 and is also close to the original input. The critical part of the code is shown below:

\begin{Verbatim}[frame=single]
adv_x = x - torch.clamp(x - (x_now - iter_eps * x_now.grad.sign()), -eps, eps)
\end{Verbatim}

In each iteration, I first compute the FGSM-style perturbation and then project the perturbed example back to the $L_\infty$ ball of radius $\epsilon$ centered at the original input. However, this time the attack still fails to generate an adversarial example that is classified as class 1.

Notice that the PGD method and the FGSM method only take account of the sign of the gradient to decide the next step, I try to use the actual gradient instead. The critical part of the code is shown below:

\begin{Verbatim}[frame=single]
adv_x = x - torch.clamp(x - (x_now - iter_eps * x_now.grad), -eps, eps)
\end{Verbatim}

This time the attack succeeds. The output is as follows:

\begin{Verbatim}[frame=single]
Original Input:  tensor([[0.6584, 0.8175, 0.1262, 0.0541, 0.2268, 0.8405, 0.5393, 0.9798, 
        0.8597, 0.5977]], requires_grad=True)
Adversarial Example:  tensor([[ 0.9066,  0.5376,  0.0899,  0.5731,  0.1653,  1.2485, 
        -0.2812, 1.7239, 0.7405,  0.3552]], grad_fn=<SubBackward0>)
New Class:  1
Norm:  tensor(0.8205)
\end{Verbatim}

By applying the original gradient instead of its sign, I can generate an adversarial example that is classified as class 1 and has an $L_\infty$ norm of perturbation equal to 0.8205, which is smaller than previous 1.1400.

\newpage
\section{Single and Multi-Norm Robustness with PGD attack}

\subsection{}

First, I implement the untargeted $L_\infty$ and $L_2$ Projected Gradient Descent (PGD) attack. For the $L_\infty$ PGD attack, I apply the update rule of the FGSM attack in each iteration, followed by a projection step to ensure that the adversarial example remains within the $L_\infty$ ball of radius $\epsilon$ centered at the original input. Finally, I clip the adversarial example to ensure that its pixel values are between 0 and 1 so that it is a valid image. 

\begin{Verbatim}[frame=single]
adv_x = adv_x + eps_step * adv_x.grad.sign()
delta = torch.clamp(adv_x - x, min=-eps, max=eps)
adv_x = torch.clamp(x + delta, min=0, max=1).detach()  
\end{Verbatim}

The process for the $L_2$ PGD attack is similar, except that I normalize the gradient by its $L_2$ norm before applying it to update the adversarial example, as instructed in the homework description. After that, I project the adversarial example back to the $L_2$ ball and clip it to ensure valid pixel values.

\begin{Verbatim}[frame=single]
grad_norm = (torch.linalg.vector_norm(grad.view(batch_size, -1), dim=1) + eps_for_division)
        .view(batch_size, 1, 1, 1)
adv_x = adv_x + (eps_step * grad) / grad_norm
delta = adv_x - x
delta_norm = torch.linalg.vector_norm(delta.view(batch_size, -1), dim=1)
        .view(batch_size, 1, 1, 1)
factor = torch.clamp(eps / delta_norm, max=1.0)
delta = delta * factor
adv_x = torch.clamp(x + delta, min=0, max=1).detach()
\end{Verbatim}

The results of above two attacks on the three pre-trained models are as follows, with the results without any attack included for reference.

\begin{table}[htbp]
\begin{tabularx}{\textwidth}{|>{\hsize=2\hsize}X|X|X|X|X|}
\hline
                                   & $L_\infty$ Attack & $L_2$ Attack & No Attack \\ \hline
Pretrained $L_\infty$-robust Model & 50.63\%           & 45.95\%      & 82.80\%   \\ \hline
Pretrained $L_2$-robust Model      & 29.11\%           & 53.24\%      & 88.75\%   \\ \hline
Pretrained RAMP Model              & 48.93\%           & 59.25\%      & 81.19\%   \\ \hline
\end{tabularx}
\caption{Robustness of Pretrained Models Against Single Attacks}
\label{tab:robustness_comparison}
\end{table}

From the results, we can see that the pretrained $L_\infty$-robust model and the pretrained $L_2$-robust model demonstrate strong robustness against the attack corresponding to what they are trained for, but they are vulnerable to the other type of attack. The pretrained RAMP model, on the other hand, shows a more balanced robustness against both types of attacks, and its performance under both attacks is close to, or even better than, the best performance of the other two models.


\newpage
\subsection{}

First, I implement the multi-norm PGD robustness analysis as follows:
\begin{Verbatim}[frame=single]
out = model(x_adv_linf)
pred_linf = torch.max(out, dim=1)[1]
out = model(x_adv_l2)
pred_l2 = torch.max(out, dim=1)[1]

# Count as correct only if both predictions are correct
tot_acc += ((pred_linf == y_batch) & (pred_l2 == y_batch)).sum().item()
tot_test += y_batch.size(0) 
\end{Verbatim}

The results are as follows\footnote{In the starter code, it is suggested that we use $\epsilon=0.5$ for the $L_2$ PGD attack. However, this is different from the $\epsilon=0.75$ used in the single-norm robustness analysis. To ensure a fair comparison, I use $\epsilon=0.75$ for the $L_2$ PGD attack in the multi-norm robustness analysis as well.}:
\begin{table}[htbp]
\begin{tabularx}{\textwidth}{|>{\hsize=1.3\hsize}X|>{\hsize=0.8\hsize}X|>{\hsize=0.6\hsize}X|>{\hsize=0.6\hsize}X|>{\hsize=0.7\hsize}X|}
\hline
                                   & Mult-norm Attack & $L_\infty$ Attack & $L_2$ Attack & No Attack \\ \hline
Pretrained $L_\infty$-robust Model & 44.62\%                               & 50.63\%           & 45.95\%      & 82.80\%   \\ \hline
Pretrained $L_2$-robust Model      & 29.11\%                               & 29.11\%           & 53.24\%      & 88.75\%   \\ \hline
Pretrained RAMP Model              & 48.92\%                               & 48.93\%           & 59.25\%      & 81.19\%   \\ \hline
\end{tabularx}
\caption{Robustness of Pretrained Models Against Multi-Norm Attacks}
\label{tab:multi_norm_robustness}
\end{table}

From the data, we can see that the performance of all three models degrades under the multi-norm attack compared to the single-norm attacks, which is expected because a model has to defend against two types of attacks simultaneously to score the accuracy. 

The data also shows that the pretrained RAMP model trades some performance for better robustness. It has a lower accuracy without any attack, but it achieves the best robustness under both single-norm and multi-norm attacks. 



\newpage
\section{Adversarial Robustness Against Unseen Adversaries}

\end{document}
